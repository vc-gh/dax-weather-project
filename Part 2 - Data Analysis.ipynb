{
    "cells": [
        {
            "source": "# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='e4c21489-ad65-4dda-8449-a49a5e577e55', project_access_token='p-6de72195a1804207206130fa387f84906969db5e')\n",
            "cell_type": "code",
            "execution_count": null,
            "outputs": [],
            "metadata": {}
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "# Exploratory Data Analysis of NOAA Weather Data \n\nThis notebook relates to the NOAA Weather Dataset - JFK Airport (New York). The dataset contains 114,546 hourly observations of 12 local climatological variables (such as temperature and wind speed) collected at JFK airport. This dataset can be obtained for free from the IBM Developer [Data Asset Exchange](https://developer.ibm.com/exchanges/data/all/jfk-weather-data/).\n\nIn this notebook we visualize and analyze the weather time-series dataset.\n\n### Table of Contents:\n* [0. Prerequisites](#cell0)\n* [1. Read the Cleaned Data](#cell1)\n* [2. Visualize the Data](#cell2)\n* [3. Analyze Trends in the Data](#cell3)\n* [Authors](#authors)\n\n\n<a id=\"cell0\"></a>\n### 0. Prerequisites\n\nBefore you run this notebook complete the following steps:\n- Insert a project token\n- Install and import required packages\n\n#### Insert a project token\n\nWhen you import this project from the Watson Studio Gallery, a token should be automatically generated and inserted at the top of this notebook as a code cell such as the one below:\n\n```python\n# @hidden_cell\n# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\nfrom project_lib import Project\nproject = Project(project_id='YOUR_PROJECT_ID', project_access_token='YOUR_PROJECT_TOKEN')\npc = project.project_context\n```\n\nIf you do not see the cell above, follow these steps to enable the notebook to access the dataset from the project's resources:\n\n* Click on `More -> Insert project token` in the top-right menu section\n\n![ws-project.mov](https://media.giphy.com/media/jSVxX2spqwWF9unYrs/giphy.gif)\n\n* This should insert a cell at the top of this notebook similar to the example given above.\n\n  > If an error is displayed indicating that no project token is defined, follow [these instructions](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/token.html?audience=wdp&context=data).\n\n* Run the newly inserted cell before proceeding with the notebook execution below\n\n#### Import required packages\n\nInstall and import the required packages:\n\n* pandas\n* matplotlib\n* seaborn\n* numpy"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Installing packages needed for data processing and visualization\n!pip install pandas matplotlib seaborn numpy ",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Importing the packages\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pandas import DataFrame as df\nfrom matplotlib import pyplot as plt\nplt.rcParams['figure.dpi'] = 160",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"cell1\"></a>\n\n### 1. Read the Cleaned Data\n\nWe start by reading in the cleaned dataset that was created in the project notebook `Part 1 - Data Cleaning`. \n\n*Note* if you haven't yet run that notebook, do that first otherwise the cells below will not work."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "def get_file_handle(fname):\n    # Project data path for the raw data file\n    data_path = project.get_file(fname)\n    data_path.seek(0)\n    return data_path\n\n# Using pandas to read the data \n# Since the `DATE` column consists date-time information, we use Pandas parse_dates keyword for easier data processing\ndata_path = get_file_handle('jfk_weather_cleaned.csv')\ndata = pd.read_csv(data_path, parse_dates=['DATE'])\n# Set date index\ndata = data.set_index(pd.DatetimeIndex(data['DATE']))\ndata.drop(['DATE'], axis=1, inplace=True)\ndata.head()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"cell2\"></a>\n\n### 2. Visualize the Data\n\nIn this section we visualize a few sections of the data, using `matplotlib`'s `pyplot` module. \n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Columns to visualize\nplot_cols = ['dry_bulb_temp_f', 'relative_humidity', 'wind_speed', 'station_pressure', 'precip']",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Quick Peek at the Data\n\nWe first visualize all the data we have to get a rough idea about how the data looks like. \n\nAs we can see in the plot below, the hourly temperatures follow a clear seasonal trend. Wind speed, pressure, humidity and precipitation data seem to have much higher variance and randomness.\n\nIt might be more meaningful to make a model to predict temperature, rather than some of the other more noisy data columns. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Quick overview of columns\nplt.figure(figsize=(30, 12))\ni = 1\nfor col in plot_cols:\n    plt.subplot(len(plot_cols), 1, i)\n    plt.plot(data[col].values)\n    plt.title(col)\n    i += 1\nplt.subplots_adjust(hspace=0.5)\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Feature Dependencies\n\nNow we explore how the features (columns) of our data are related to each other. This helps in deciding which features to use when modelling a classifier or regresser. \nWe ideally want independent features to be classified independently and likewise dependent features to be contributing to the same model. \n\nWe can see from the correlation plots how some features are somewhat correlated and could be used as additional data (perhaps for augmenting) when training a classifier. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot correlation matrix\nf, ax = plt.subplots(figsize=(7, 7))\ncorr = data[plot_cols].corr()\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True), square=True, ax=ax);",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Additionally we also visualize the joint distrubitions in the form of pairplots/scatter plots to see (qualitatively) the way in which these features are related in more detail over just the correlation.\nThey are essentially 2D joint distributions in the case of off-diagonal subplots and the histogram (an approximation to the probability distribution) in case of the diagonal subplots."
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "# Plot pairplots\nsns.pairplot(data[plot_cols])",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"cell3\"></a>\n\n### 3. Analyze Trends in the Data\n\nNow that we have explored the whole dataset and the features on a high level, let us focus on one particular feature - `dry_bulb_temp_f`, the dry bulb temperature in degrees Fahrenheit. This is what we mean when we refer to \"air temperature\". This is the most common feature used in temperature prediction, and here we explore it in further detail. \n\nWe first start with plotting the data for all 9 years in monthly buckets then drill down to a single year to notice (qualitatively) the overall trend in the data. We can see from the plots that every year has roughly a sinousoidal nature to the temperature with some anomalies around 2013-2014. Upon further drilling down we see that each year's data is not the smooth sinousoid but rather a jagged and noisy one. But the overall trend still is a sinousoid. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plt.figure(figsize=(15,7))\n\nTEMP_COL = 'dry_bulb_temp_f'\n# Plot temperature data converted to a monthly frequency \nplt.subplot(1, 2, 1)\ndata[TEMP_COL].asfreq('M').plot()  \nplt.title('Monthly Temperature')\nplt.ylabel('Temperature (F)')\n\n# Zoom in on a year and plot temperature data converted to a daily frequency \nplt.subplot(1, 2, 2)\ndata['2017'][TEMP_COL].asfreq('D').plot()  \nplt.title('Daily Temperature in 2017')\nplt.ylabel('Temperature (F)')\n\nplt.tight_layout()\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Next, we plot the change (delta) in temperature and notice that it is lowest around the middle of the year. That is expected behaviour as the gradient of the sinousoid near it's peak is zero. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plt.figure(figsize=(15,7))\n\n# Plot percent change of daily temperature in 2017\nplt.subplot(1, 2, 1)\ndata['2017'][TEMP_COL].asfreq('D').div(data['2017'][TEMP_COL].asfreq('D').shift()).plot()\nplt.title('% Change in Daily Temperature in 2017')\nplt.ylabel('% Change')\n\n# Plot absolute change of temperature in 2017 with daily frequency\nplt.subplot(1, 2, 2)\ndata['2017'][TEMP_COL].asfreq('D').diff().plot()\nplt.title('Absolute Change in Daily Temperature in 2017')\nplt.ylabel('Temperature (F)')\n\nplt.tight_layout()\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "Finally we apply some smoothing to the data in the form of a rolling/moving average. This is the simplest form of de-noising the data. As we can see from the plots, the average (plotted in blue) roughly traces the sinousoid and is now much smoother. This can improve the accuracy of a regression model trained to predict temperatures within a reasonable margin of error. "
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "plt.figure(figsize=(15,7))\n\n# Plot rolling mean of temperature \nplt.subplot(1, 2, 1)\ndata['2017'][TEMP_COL].rolling('5D').mean().plot(zorder=2) # Rolling average window is 5 days\ndata['2017'][TEMP_COL].plot(zorder=1)\nplt.legend(['Rolling','Temp'])\nplt.title('Rolling Avg in Hourly Temperature in 2017')\nplt.ylabel('Temperature (F)')\n\n# Plot rolling mean of temperature\nplt.subplot(1, 2, 2)\ndata['2017-01':'2017-03'][TEMP_COL].rolling('2D').mean().plot(zorder=2) # Rolling average window is 2 days\ndata['2017-01':'2017-03'][TEMP_COL].plot(zorder=1)\nplt.legend(['Rolling','Temp'])\nplt.title('Rolling Avg in Hourly Temperature in Winter 2017')\nplt.ylabel('Temperature (F)')\n\nplt.tight_layout()\nplt.show()",
            "execution_count": null,
            "outputs": []
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "#### Next steps\n\n- Close this notebook.\n- Open the `Part 3 - Time Series Forecasting` notebook to create time-series models to forecast temperatures."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "<a id=\"authors\"></a> \n### Authors\n\nThis notebook was created by the [Center for Open-Source Data & AI Technologies](http://codait.org).\n<br><br>\n\nCopyright \u00a9 2019 IBM. This notebook and its source code are released under the terms of the MIT License.\n<br><br>\n<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.6",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.6.9",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}